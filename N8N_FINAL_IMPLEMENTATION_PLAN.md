# N8N Final Implementation Plan

## Current N8N Webhook Flow (Changed Today)

### Previous Flow (3 days ago):
- Vercel calls n8n webhook 
- n8n responds directly to the webhook call
- Response came back synchronously

### Current Flow (Changed Today):
1. **Vercel ‚Üí n8n webhook**
   - Authentication: `N8N_WEBHOOK_SECRET_KEY`
   - n8n receives the webhook call from Vercel

2. **n8n processes** (1-12+ minutes with long-running tools)

3. **n8n ‚Üí Vercel callback**
   - POST to `/api/n8n-callback`
   - Authentication: `N8N_CALLBACK_SECRET_KEY`
   - n8n sends response back when processing complete

## Authentication Keys:
- **N8N_WEBHOOK_SECRET_KEY**: Vercel ‚Üí n8n authentication
- **N8N_CALLBACK_SECRET_KEY**: n8n ‚Üí Vercel authentication

## Current App Behavior & Problem

### Current "Thinking..." Animation:
- **All models** (n8n and non-n8n) show "Thinking..." animation when waiting for LLM response
- This is the standard Vercel AI SDK thinking state with animation
- **NOT a database message** - it's a UI state

### The Problem:
- For **n8n models**: Animation times out after ~60 seconds
- This is the entire origin of the problem we're solving
- n8n workflows with long-running tools take 1-12+ minutes

## Desired Behavior

### Short Term Solution:
- Show the **SAME EXACT "Thinking..." animation** (not database pollution)
- Keep animation running until n8n POST response arrives at `/api/n8n-callback`
- Extend timeout to **10-15 minutes** instead of 60 seconds
- When n8n response arrives, replace thinking animation with actual response

### Long Term Possibilities:
- **Streaming**: Convert to streaming responses from n8n
- **Multiple POST responses**: n8n sends multiple callbacks based on different tools used
- **Migration**: Move n8n workflows/tools into this codebase directly (off n8n platform)

## N8N Platform Limitations & Constraints

### N8N Streaming Capability:
- **n8n CANNOT stream responses natively**
- n8n has JavaScript/Python execution in code nodes (function nodes)
- **We will NOT use hacky solutions** with code nodes for streaming
- **Assumption**: n8n will run 1s to 10 minutes per message and POST only when complete

### Vercel Timeout Challenge:
- Vercel times out at 60 seconds
- n8n workflows can take up to 10 minutes
- **Must avoid timeout while maintaining thinking animation**

### Potential Solutions for Timeout:
1. **Keep stream alive**: Send periodic messages from system to itself during wait
2. **Different async paradigm**: Use alternative approach to maintain connection
3. **Extend timeout**: Configure longer timeout if possible
4. **Background processing**: Complete disconnect from stream, rely purely on callback

## IMMEDIATE ISSUE - BUILD FAILURE BLOCKING DEPLOYMENT

### TypeScript Error:
```
./app/(chat)/api/chat/route.ts:34:35
Type error: Type 'string | undefined' is not assignable to type 'string'.
Type 'undefined' is not assignable to type 'string'.

const client = new MemoryClient({ apiKey: process.env.MEM0_API_KEY });
```

### Problem:
- `process.env.MEM0_API_KEY` can be `undefined` (TypeScript doesn't know it exists)
- MemoryClient constructor expects `string`
- **BLOCKING ALL DEPLOYMENTS AND TESTING**
- **Note**: MEM0_API_KEY exists in both Vercel and local .env

### Fix Applied:
- Added fallback: `process.env.MEM0_API_KEY || ''`
- Satisfies TypeScript while environment variable actually exists
- ‚úÖ **FIXED** - Build now completes successfully 

## TEST RESULTS - May 23, 2025

### Test Details:
- **Date/Time**: May 23, 2025 at ~13:39 WITA
- **Vercel Deployment**: F1oUVSzLW  
- **Git Commit**: 4a810ec
- **Test**: Messaged n8n model in new Vercel deployment

### Frontend Behavior:
- ‚ùå **UI immediately responded with error toast** (before n8n could possibly respond to webhook or POST)
- ‚ùå **No response ultimately arrived in Vercel UI**

### N8N Backend Results:
- ‚úÖ **Webhook received successfully**
- ‚úÖ **HTTP POST sent successfully** 
- ‚ùå **Response from POST was random HTML, not JSON**

### N8N POST Details:
**Target URL**: `https://ai.chrisyork.co/api/n8n-callback`

**POST Body** (‚úÖ Correct JSON format):
```json
{
  "chatId": "2785f9c8-945c-41f6-afed-9c1c14645d8a",
  "responseMessage": "Hi Chris, what can I do for you?"
}
```

**Authorization Header** (‚úÖ Correct):
```
Authorization: Bearer {value of N8N_CALLBACK_SECRET_KEY}
```

**POST Response** (‚ùå HTML instead of JSON):
```html
<!DOCTYPE html><html lang="en" class="__variable_1b340f __variable_ae1979">
<head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<link rel="stylesheet" href="/_next/static/css/56afe357d4bb9194.css?dpl=dpl_F1oUVSzLWT5NAGmMpK2DotTnaamf" data-precedence="next"/>
[...truncated Next.js HTML page...]
```

### Vercel Logs Analysis:
- ‚úÖ **`/api/chat` route worked correctly**:
  - Saved user message successfully
  - Triggered n8n workflow successfully  
  - Returned success immediately
- ‚ùå **`/api/n8n-callback` endpoint issue**:
  - n8n sent correct JSON payload
  - Endpoint returned HTML page instead of JSON response
  - Suggests authentication failure or API route error

### Root Cause Analysis:
1. **Frontend error toast**: Likely caused by `/api/n8n-callback` endpoint failure
2. **HTML response**: `/api/n8n-callback` is not functioning properly - returning Next.js error page
3. **Authentication**: May be failing validation of `N8N_CALLBACK_SECRET_KEY`
4. **API Route**: `/api/n8n-callback` may have TypeScript errors or missing implementation

### Next Steps:
1. **Debug `/api/n8n-callback` endpoint** - check for errors, authentication logic
2. **Verify `N8N_CALLBACK_SECRET_KEY` environment variable** in Vercel deployment
3. **Add proper error handling** and JSON responses to callback endpoint
4. **Test authentication flow** between n8n and Vercel callback 

### Potential Implementation Directions

## MINIMAL SOLUTIONS (No Psychotic Over-Engineering)

### Solution A: Simple Frontend Polling ‚≠ê **MINIMAL & RELIABLE**

**How it works:**
1. `/api/chat` saves user message, triggers n8n webhook, returns success immediately  
2. Frontend shows thinking animation and polls for new messages every 3 seconds
3. `/api/n8n-callback` saves assistant message to database
4. Polling detects new message and updates UI, stops thinking animation

**Code changes needed:**
```typescript
// Frontend: Add simple polling in chat component
useEffect(() => {
  if (isWaitingForN8n) {
    const interval = setInterval(async () => {
      const response = await fetch(`/api/chat/${chatId}/messages`);
      const messages = await response.json();
      // Check if new assistant message appeared
      // If yes, update UI and stop polling
    }, 3000);
    return () => clearInterval(interval);
  }
}, [isWaitingForN8n]);

// Backend: Simple messages API route
// GET /api/chat/[id]/messages - returns messages for chat
```

**Total implementation:** ~20 lines of code
**Works with:** No external dependencies, uses existing database
**Testing:** Just test locally without n8n callback

**Pros:**
- Dead simple
- Completely reliable  
- Easy to test and debug
- No complex state management
- Works with any response time

**Cons:**
- 3-second delay to show response (acceptable)
- Extra API calls (minimal impact)

### Solution B: Simple Stream Heartbeat (If Polling Not Acceptable)

**How it works:**
1. `/api/chat` starts streaming, triggers n8n webhook
2. Send heartbeat every 30 seconds to keep Vercel connection alive
3. Store active streams in simple `Map<chatId, stream>`
4. `/api/n8n-callback` pushes message through existing stream

**Code changes needed:**
```typescript
// Simple global stream storage
const activeStreams = new Map();

// /api/chat: Register stream and heartbeat
const heartbeat = setInterval(() => {
  dataStream.writeData({ type: 'heartbeat' });
}, 30000);
activeStreams.set(chatId, { dataStream, heartbeat });

// /api/n8n-callback: Use existing stream
const { dataStream, heartbeat } = activeStreams.get(chatId);
dataStream.writeMessageAnnotation({ message: responseMessage });
clearInterval(heartbeat);
activeStreams.delete(chatId);
```

**Total implementation:** ~30 lines of code
**Real-time response, no polling**

## RECOMMENDATION: Use Solution A (Polling)

**Why polling is better:**
- Simpler to implement and debug
- More reliable (no connection management)
- Easier to test (don't need n8n callback)
- Less likely to break
- 3-second delay is acceptable for 1-12 minute workflows

**Implementation priority:**
1. ‚úÖ Fix middleware auth (DONE)
2. ‚úÖ Fix response format (DONE)  
3. üîÑ Add simple polling (20 minutes)
4. üîÑ Test without n8n (5 minutes)
5. üîÑ Test with real n8n (5 minutes)

**Total implementation time:** 30 minutes

## CRITICAL ISSUES IDENTIFIED (May 23, 2025)

### Issue 1: Middleware Authentication Blocking
**Problem**: `/api/n8n-callback` returns HTML because Clerk middleware redirects to `/sign-in`
**Root Cause**: `/api/n8n-callback` not in public routes list in `middleware.ts`
**Status**: ‚úÖ **FIXED** - Added `/api/n8n-callback` to public routes

### Issue 2: Frontend Response Format Mismatch  
**Problem**: UI shows immediate error toast before n8n can respond
**Root Cause**: `/api/chat` returns `new Response('OK')` but Assistant UI expects streaming response
**Status**: ‚úÖ **FIXED** - Changed to `createDataStreamResponse()` format

### Issue 3: Thinking Animation Lifecycle (REMAINING MAJOR ISSUE)
**Problem**: Current implementation shows "Thinking..." for ~100ms then disappears
**Root Cause**: Stream ends immediately after triggering n8n webhook
**Impact**: User sees nothing for 1-12 minutes until n8n responds
**Status**: ‚ùå **NEEDS SOLUTION**

### Issue 4: n8n Response UI Integration (REMAINING MAJOR ISSUE)  
**Problem**: n8n callback saves message to database but doesn't update chat UI
**Root Cause**: Frontend has no way to know new message was added
**Impact**: Response exists in database but never appears in chat
**Status**: ‚ùå **NEEDS SOLUTION**

## RESEARCHED SOLUTIONS (Based on Current Documentation)

### Solution A: Keep Stream Alive with Periodic Heartbeats ‚≠ê **RECOMMENDED**

**How it works:**
1. `/api/chat` starts streaming response for n8n models
2. Send heartbeat every 30-45 seconds to keep stream alive 
3. Store active stream reference associated with chatId
4. When `/api/n8n-callback` receives response, push final message through existing stream
5. End stream with complete response

**Technical Implementation:**
```typescript
// In /api/chat for n8n models:
return createDataStreamResponse({
  execute: async (dataStream) => {
    // Store stream reference for callback
    streamRegistry.set(chatId, dataStream);
    
    // Send heartbeats every 30s to keep stream alive
    const heartbeatInterval = setInterval(() => {
      dataStream.writeData({ type: 'heartbeat', timestamp: Date.now() });
    }, 30000);
    
    // Stream will be completed by n8n-callback
    // Don't end stream here - let callback handle it
  }
});

// In /api/n8n-callback:
const dataStream = streamRegistry.get(chatId);
if (dataStream) {
  // Send final message through existing stream
  dataStream.writeMessageAnnotation({ 
    type: 'assistant-message',
    content: responseMessage 
  });
  clearInterval(heartbeatInterval);
  streamRegistry.delete(chatId);
}
```

**Pros:**
- Uses standard Vercel AI SDK patterns
- Keeps thinking animation alive naturally
- No database pollution or fake messages
- Works within Vercel 60s timeout (heartbeats reset connection)
- Seamless user experience

**Cons:**
- Requires in-memory stream registry
- More complex state management
- Potential memory leaks if not cleaned up

### Solution B: Frontend Polling with Chat Revalidation

**How it works:**
1. `/api/chat` saves user message, triggers n8n, returns success immediately
2. Frontend shows thinking animation and polls for new messages
3. `/api/n8n-callback` saves assistant message and revalidates cache
4. Polling detects new message and updates UI

**Technical Implementation:**
```typescript
// In /api/chat for n8n models:
// Current implementation (trigger webhook, return immediately)
// Add status tracking to Chat table

// New polling API:
// GET /api/chat/[id]/messages?since=timestamp
// Returns new messages since timestamp

// Frontend polling:
const pollForNewMessages = async () => {
  const response = await fetch(`/api/chat/${chatId}/messages?since=${lastMessageTime}`);
  const newMessages = await response.json();
  if (newMessages.length > 0) {
    // Update chat UI with new messages
    // Stop thinking animation
  }
};
```

**Pros:**
- Simple implementation
- No stream management complexity
- Clean separation of concerns
- Reliable delivery

**Cons:**
- Polling overhead (3-5 second intervals)
- Not real-time responsive
- More API calls

### Solution C: WebSocket/Server-Sent Events (SSE) for Real-Time Updates

**How it works:**
1. Establish WebSocket or SSE connection per chat session
2. `/api/chat` triggers n8n webhook and returns immediately
3. `/api/n8n-callback` broadcasts message through WebSocket/SSE
4. Frontend receives real-time update and displays message

**Technical Implementation:**
```typescript
// WebSocket approach requires custom server setup
// SSE approach could use Vercel's streaming

// In /api/chat:
// Store chatId -> connectionId mapping

// In /api/n8n-callback:
// Broadcast to specific chat connection
```

**Pros:**
- True real-time updates
- Efficient (no polling)
- Scalable for multiple concurrent chats

**Cons:**
- Complex infrastructure on Vercel
- WebSocket support limitations
- Connection management overhead

### Solution D: External Message Queue (Fallback)

**How it works:**
1. Use external service (Upstash Redis, Ably, etc.) for pub/sub
2. `/api/chat` triggers n8n and subscribes to response channel
3. `/api/n8n-callback` publishes message to channel
4. Frontend listens to channel for updates

**Pros:**
- Completely decoupled
- Highly reliable
- Handles connection failures gracefully

**Cons:**
- Requires external service
- Additional infrastructure cost
- More complexity

## RECOMMENDED APPROACH: Solution A (Stream Heartbeats)

**Why this is best:**
1. **Works with existing Assistant UI patterns** - uses standard streaming response
2. **Keeps thinking animation alive naturally** - no UI state hacks needed  
3. **Handles Vercel timeout** - heartbeats prevent 60s timeout
4. **Clean user experience** - seamless like other AI models
5. **Minimal external dependencies** - only requires in-memory stream registry

**Implementation Priority:**
1. **Fix middleware authentication** ‚úÖ DONE
2. **Fix response format** ‚úÖ DONE  
3. **Implement stream heartbeat system** ‚è≥ NEXT
4. **Update n8n-callback to use existing streams** ‚è≥ NEXT
5. **Add cleanup and error handling** ‚è≥ NEXT

**Estimated Development Time:** 2-4 hours

**Testing Strategy:**
1. Test with short n8n workflows (1-2 minutes)
2. Test with long n8n workflows (5-10 minutes)  
3. Test connection failures and cleanup
4. Test multiple concurrent n8n requests

## DETAILED IMPLEMENTATION PLAN: Solution A (Stream Heartbeats)

### Phase 1: Stream Registry Setup

#### Step 1.1: Create Stream Registry Module
**File:** `lib/stream-registry.ts`
```typescript
interface StreamEntry {
  dataStream: DataStreamWriter;
  heartbeatInterval: NodeJS.Timeout;
  createdAt: Date;
  chatId: string;
}

class StreamRegistry {
  private streams = new Map<string, StreamEntry>();
  
  register(chatId: string, dataStream: DataStreamWriter): void {
    // Clear any existing stream for this chat
    this.cleanup(chatId);
    
    // Start heartbeat
    const heartbeatInterval = setInterval(() => {
      try {
        dataStream.writeData({ 
          type: 'heartbeat', 
          timestamp: Date.now(),
          chatId 
        });
        console.log(`[StreamRegistry] Heartbeat sent for chat ${chatId}`);
      } catch (error) {
        console.error(`[StreamRegistry] Heartbeat failed for chat ${chatId}:`, error);
        this.cleanup(chatId);
      }
    }, 30000); // 30 seconds
    
    this.streams.set(chatId, {
      dataStream,
      heartbeatInterval,
      createdAt: new Date(),
      chatId
    });
    
    console.log(`[StreamRegistry] Registered stream for chat ${chatId}`);
  }
  
  get(chatId: string): DataStreamWriter | null {
    const entry = this.streams.get(chatId);
    return entry?.dataStream || null;
  }
  
  cleanup(chatId: string): void {
    const entry = this.streams.get(chatId);
    if (entry) {
      clearInterval(entry.heartbeatInterval);
      this.streams.delete(chatId);
      console.log(`[StreamRegistry] Cleaned up stream for chat ${chatId}`);
    }
  }
  
  // Cleanup stale streams (older than 15 minutes)
  cleanupStale(): void {
    const fifteenMinutesAgo = new Date(Date.now() - 15 * 60 * 1000);
    
    for (const [chatId, entry] of this.streams.entries()) {
      if (entry.createdAt < fifteenMinutesAgo) {
        console.log(`[StreamRegistry] Cleaning up stale stream for chat ${chatId}`);
        this.cleanup(chatId);
      }
    }
  }
  
  // Get active stream count for monitoring
  getActiveCount(): number {
    return this.streams.size;
  }
}

export const streamRegistry = new StreamRegistry();

// Cleanup stale streams every 5 minutes
setInterval(() => {
  streamRegistry.cleanupStale();
}, 5 * 60 * 1000);
```

**Checklist:**
- [ ] Create `lib/stream-registry.ts` file
- [ ] Implement StreamEntry interface
- [ ] Implement StreamRegistry class with all methods
- [ ] Add logging for debugging
- [ ] Add stale stream cleanup mechanism

#### Step 1.2: Update Chat Route for n8n Models
**File:** `app/(chat)/api/chat/route.ts`

**Changes needed:**
```typescript
// Add import at top
import { streamRegistry } from '@/lib/stream-registry';

// Replace the n8n section around line 360:
if (selectedModelInfo?.isN8n) {
  console.log(`[API Route] Triggering n8n workflow for chat ${finalChatId}`);

  const webhookUrl = n8nWebhookUrls[selectedChatModel];
  if (!webhookUrl) {
    console.error(`Webhook URL for n8n assistant "${selectedChatModel}" is not configured.`);
    return new Response('Assistant configuration error', { status: 500 });
  }

  // Build n8n payload
  const payload = {
    chatId: finalChatId,
    userId,
    messageId: userMessage.id,
    userMessage: typeof userMessage.content === 'string' 
      ? userMessage.content 
      : JSON.stringify(userMessage.content),
    userMessageParts: userMessage.parts,
    userMessageDatetime: userMessage.createdAt,
    history: messages.slice(0, -1),
    ...(tokenResult.token && { google_token: tokenResult.token }),
  };

  console.log('[API Route] n8n payload:', JSON.stringify(payload, null, 2));

  // Return streaming response that stays alive
  return createDataStreamResponse({
    execute: async (dataStream) => {
      // Register stream for callback access
      streamRegistry.register(finalChatId, dataStream);
      
      // Fire n8n webhook without awaiting response
      fetch(webhookUrl, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          ...(process.env.N8N_WEBHOOK_SECRET_KEY && {
            Authorization: `Bearer ${process.env.N8N_WEBHOOK_SECRET_KEY}`,
          }),
        },
        body: JSON.stringify(payload),
      })
      .then((resp) => {
        console.log('[API Route] n8n webhook responded with status', resp.status);
        if (!resp.ok) {
          console.error('[API Route] n8n webhook failed:', resp.statusText);
          // Send error through stream
          dataStream.writeData({ 
            type: 'error', 
            message: `n8n webhook failed: ${resp.statusText}` 
          });
          streamRegistry.cleanup(finalChatId);
        }
      })
      .catch((error) => {
        console.error('[API Route] Error triggering n8n webhook:', error);
        // Send error through stream
        dataStream.writeData({ 
          type: 'error', 
          message: `n8n webhook error: ${error.message}` 
        });
        streamRegistry.cleanup(finalChatId);
      });

      console.log('[API Route] n8n workflow triggered, stream registered with heartbeats');
      
      // Don't end the stream here - let the callback handle it
      // The stream will stay alive with heartbeats until callback completes
    },
    onError: (error) => {
      console.error('[API Route] Stream error:', error);
      streamRegistry.cleanup(finalChatId);
      return `Stream error: ${error.message}`;
    },
  });
}
```

**Checklist:**
- [ ] Add streamRegistry import
- [ ] Replace n8n handling section
- [ ] Add stream registration
- [ ] Add webhook error handling through stream
- [ ] Add onError handler for stream cleanup
- [ ] Remove immediate stream ending

### Phase 2: Update n8n Callback

#### Step 2.1: Modify n8n Callback Route
**File:** `app/(chat)/api/n8n-callback/route.ts`

**Complete replacement:**
```typescript
import { NextResponse } from 'next/server';
import { saveMessages } from '@/lib/db/queries';
import { revalidateTag } from 'next/cache';
import { streamRegistry } from '@/lib/stream-registry';

export async function POST(request: Request) {
  console.log('[n8n-callback] POST request received');

  try {
    // Verify callback secret if configured
    const callbackSecret = process.env.N8N_CALLBACK_SECRET_KEY;
    console.log('[n8n-callback] Checking auth with secret:', callbackSecret ? 'present' : 'missing');

    if (callbackSecret) {
      const authHeader = request.headers.get('authorization');
      console.log('[n8n-callback] Auth header:', authHeader ? 'present' : 'missing');

      if (!authHeader || authHeader !== `Bearer ${callbackSecret}`) {
        console.error('[n8n-callback] Invalid or missing authorization header');
        console.error('[n8n-callback] Expected:', `Bearer ${callbackSecret}`);
        console.error('[n8n-callback] Received:', authHeader);
        return NextResponse.json({ error: 'Unauthorized' }, { status: 401 });
      }
    }

    const body = await request.json();
    console.log('[n8n-callback] Request body:', JSON.stringify(body, null, 2));

    const {
      chatId,
      responseMessage,
      parts,
    }: {
      chatId: string;
      responseMessage: string;
      parts?: Array<{ type: string; text: string }>;
    } = body;

    console.log('[n8n-callback] Received callback for chat:', chatId);
    console.log('[n8n-callback] Response message length:', responseMessage?.length || 0);

    // Get the active stream for this chat
    const dataStream = streamRegistry.get(chatId);
    
    if (!dataStream) {
      console.warn(`[n8n-callback] No active stream found for chat ${chatId}`);
      // Still save the message to database as fallback
    } else {
      console.log(`[n8n-callback] Found active stream for chat ${chatId}`);
    }

    // Build message parts
    const messageParts = parts && parts.length > 0 
      ? parts 
      : [{ type: 'text', text: responseMessage }];

    // Save message to database
    console.log('[n8n-callback] Saving assistant message to database...');
    const savedMessages = await saveMessages({
      messages: [
        {
          chatId: chatId,
          role: 'assistant',
          parts: messageParts,
          attachments: [],
          createdAt: new Date(),
        },
      ],
    });

    console.log('[n8n-callback] Successfully saved assistant message');

    // If we have an active stream, send the response through it
    if (dataStream) {
      try {
        // Send the assistant message through the stream
        dataStream.writeMessageAnnotation({
          type: 'assistant-response',
          chatId: chatId,
          message: {
            id: savedMessages[0]?.id, // Use the actual saved message ID
            role: 'assistant',
            content: messageParts,
            createdAt: new Date().toISOString(),
          }
        });

        // Send completion signal
        dataStream.writeData({
          type: 'completion',
          chatId: chatId,
          timestamp: Date.now()
        });

        console.log('[n8n-callback] Sent response through active stream');
        
        // Clean up the stream
        streamRegistry.cleanup(chatId);
        console.log('[n8n-callback] Cleaned up stream for chat', chatId);
      } catch (streamError) {
        console.error('[n8n-callback] Error sending through stream:', streamError);
        // Continue with database fallback
      }
    }

    // Revalidate chat cache so other clients can pick up the new message
    revalidateTag(`chat-${chatId}`);

    console.log('[n8n-callback] Successfully processed callback');

    return NextResponse.json({ 
      ok: true, 
      messageId: savedMessages[0]?.id,
      streamDelivered: !!dataStream 
    });

  } catch (error: any) {
    console.error('[n8n-callback] Error processing callback:', error);
    console.error('[n8n-callback] Error stack:', error.stack);
    return NextResponse.json(
      { error: 'Internal server error', details: error.message },
      { status: 500 },
    );
  }
}
```

**Checklist:**
- [ ] Add streamRegistry import
- [ ] Add stream lookup logic
- [ ] Add message sending through stream
- [ ] Add completion signal
- [ ] Add stream cleanup
- [ ] Add fallback database saving
- [ ] Add proper error handling
- [ ] Test response format includes streamDelivered flag

### Phase 3: Testing & Validation

#### Step 3.1: Local Testing Setup
**Create test script:** `scripts/test-n8n-flow.js`
```javascript
// Test script to simulate n8n callback
const testCallback = async (chatId, responseMessage) => {
  const response = await fetch('http://localhost:3000/api/n8n-callback', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${process.env.N8N_CALLBACK_SECRET_KEY}`,
    },
    body: JSON.stringify({
      chatId,
      responseMessage,
    }),
  });
  
  const result = await response.json();
  console.log('Callback result:', result);
};

// Usage: node scripts/test-n8n-flow.js <chatId> <message>
const chatId = process.argv[2];
const message = process.argv[3] || 'Test response from n8n';

if (!chatId) {
  console.error('Usage: node scripts/test-n8n-flow.js <chatId> <message>');
  process.exit(1);
}

testCallback(chatId, message);
```

**Testing Checklist:**
- [ ] Create test script
- [ ] Test with real n8n webhook (short workflow)
- [ ] Test with simulated long delay (10+ minutes)
- [ ] Test heartbeat functionality (check logs)
- [ ] Test stream cleanup on completion
- [ ] Test stream cleanup on error
- [ ] Test multiple concurrent streams
- [ ] Test stale stream cleanup
- [ ] Monitor memory usage during testing

#### Step 3.2: Production Validation
**Monitoring points:**
- [ ] Add stream registry metrics endpoint (`/api/debug/streams`)
- [ ] Monitor active stream count
- [ ] Monitor heartbeat success rate
- [ ] Monitor callback delivery rate
- [ ] Set up alerts for failed streams

#### Step 3.3: Error Scenarios Testing
- [ ] Test n8n webhook timeout
- [ ] Test n8n callback never arrives
- [ ] Test invalid callback payload
- [ ] Test authentication failures
- [ ] Test stream registry cleanup under load
- [ ] Test frontend behavior with heartbeats
- [ ] Test frontend behavior with completion

### Phase 4: Frontend Integration

#### Step 4.1: Frontend Heartbeat Handling
**File:** Frontend components need to handle heartbeat messages

**Expected behavior:**
- [ ] Heartbeat messages should not affect thinking animation
- [ ] Completion messages should end thinking animation
- [ ] Error messages should show error state
- [ ] Verify thinking animation stays alive during heartbeats

### Phase 5: Deployment & Monitoring

#### Step 5.1: Environment Variables
- [ ] Verify `N8N_CALLBACK_SECRET_KEY` in Vercel
- [ ] Verify `N8N_WEBHOOK_SECRET_KEY` in Vercel
- [ ] Test environment variable access in both routes

#### Step 5.2: Deployment Checklist
- [ ] Deploy with stream registry changes
- [ ] Test on Vercel staging environment
- [ ] Monitor Vercel function duration (should stay under 300s)
- [ ] Test with real n8n workflows
- [ ] Verify no memory leaks in production
- [ ] Monitor error rates

#### Step 5.3: Performance Monitoring
- [ ] Track average stream duration
- [ ] Track heartbeat delivery rate
- [ ] Track callback success rate
- [ ] Track stream cleanup efficiency
- [ ] Monitor memory usage trends

### Phase 6: Documentation & Maintenance

#### Step 6.1: Documentation
- [ ] Document stream registry architecture
- [ ] Document heartbeat mechanism
- [ ] Document error handling flows
- [ ] Document monitoring and alerting
- [ ] Update n8n webhook documentation

#### Step 6.2: Maintenance Tasks
- [ ] Set up automated stream registry monitoring
- [ ] Create runbook for stream debugging
- [ ] Set up alerts for high stream counts
- [ ] Plan for horizontal scaling if needed

## RISK MITIGATION

### Memory Leaks
**Risk:** Stream registry grows indefinitely
**Mitigation:** 
- Automatic stale stream cleanup every 5 minutes
- Maximum stream age of 15 minutes
- Monitor active stream count

### Vercel Function Timeout
**Risk:** Function times out even with heartbeats
**Mitigation:**
- Set maxDuration = 300 (5 minutes max)
- Graceful degradation to polling if stream fails
- Proper error handling and cleanup

### Callback Failures
**Risk:** n8n callback never arrives
**Mitigation:**
- Database fallback still works
- Stream cleanup handles orphaned streams
- Monitoring alerts for failed callbacks

### Concurrent Stream Limits
**Risk:** Too many concurrent streams
**Mitigation:**
- Monitor active stream count
- Set reasonable limits (e.g., 100 concurrent)
- Graceful degradation to immediate database save

## CURRENT IMPLEMENTATION ATTEMPT - SSE Heartbeats (May 23, 2025)

### What Was Implemented:
**Git Commits:**
- `Fix n8n response format: use createDataStreamResponse instead of plain text OK` 
- `Implement SSE heartbeat solution for n8n long-running callbacks`

### Code Changes Made:

#### 1. `/api/chat/route.ts` Changes:
- **Problem Fixed**: Replaced `return new Response('OK', { status: 200 })` with proper `createDataStreamResponse`
- **New Logic**: 
  - Store stream in `global.activeStreams` Map
  - Send heartbeat every 30 seconds to keep stream alive
  - Fire n8n webhook without awaiting
  - Don't end stream immediately

#### 2. `/api/n8n-callback/route.ts` Changes:  
- **New Logic**:
  - Look up stored stream from `global.activeStreams`
  - Send response through stream using `writeMessageAnnotation`
  - Clean up stream and heartbeat interval
  - Save message to database

#### 3. `middleware.ts` Fix:
- **Fixed**: Added `/api/n8n-callback` to public routes (was getting blocked by Clerk auth)

### Expected Issues:
1. **Frontend Compatibility**: Assistant UI might not handle `writeMessageAnnotation` correctly
2. **Stream Format**: The data stream format might not match what frontend expects
3. **Global State**: Using `global.activeStreams` is hacky and might not persist across Vercel serverless functions
4. **TypeScript Errors**: Global declaration might cause build issues

### Expected Test Results:
- ‚ùì **UI Error Toast**: Should be fixed (no longer returns plain text 'OK')
- ‚ùì **n8n Callback**: Should get JSON response now (auth fixed)
- ‚ùì **Thinking Animation**: Might stay forever or disappear incorrectly
- ‚ùì **Message Delivery**: Stream message might not appear in chat UI

### Test Plan:
1. Send message to n8n model
2. Check if UI shows error toast immediately
3. Check n8n logs for POST response format
4. Check if thinking animation behaves correctly
5. Check if n8n response appears in chat

### ACTUAL TEST RESULTS (May 23, 2025 - Third Test - Fixed SSE):

#### Frontend Behavior:
- ‚úÖ **No immediate error toast** (response format fixed)
- ‚úÖ **Thinking animation persists indefinitely** (stream stays alive)
- ‚ùå **Message never appears in real-time** 
- ‚úÖ **Message appears after hard refresh** (database save works)

#### Backend Behavior:
- ‚úÖ **n8n webhook triggered successfully**
- ‚úÖ **n8n callback received successfully**
- ‚ùì **Stream resolution status unknown** (need to check logs)

#### Analysis - FAILURE MODE 2: "Frontend Stream Format Issues"

**What's Working:**
- ‚úÖ **Stream persistence** - thinking animation doesn't disappear (execute function waiting)
- ‚úÖ **Database operations** - message saved correctly (appears on refresh)
- ‚úÖ **Authentication** - n8n callback reaching endpoint successfully

**What's Broken:**
- ‚ùå **Stream message delivery** - `writeMessageAnnotation` not displaying message
- ‚ùå **Stream completion** - promise not being resolved (thinking never stops)
- ‚ùå **Real-time UI updates** - frontend not receiving stream data

#### Root Cause Analysis:

**Problem 1: Stream Message Format**
- `writeMessageAnnotation` format likely incompatible with Assistant UI
- Frontend expecting different message structure
- Stream data not triggering UI message display

**Problem 2: Promise Resolution Issues**
- Promise may not be resolving properly
- Stream staying open indefinitely
- No completion signal reaching frontend

**Problem 3: Serverless State Persistence**
- Global maps may still be lost between function instances
- Promise resolvers not persisting across invocations
- State management failing on Vercel serverless

#### CONCLUSION: SSE Approach Still Fundamentally Flawed

Despite fixes, core issues remain:
- ‚ùå **Stream format incompatibility** with frontend expectations
- ‚ùå **Serverless state management** unreliable
- ‚ùå **Complex debugging** required for stream protocols
- ‚úÖ **Database fallback works perfectly** (confirmed twice)

#### IMMEDIATE NEXT STEP: IMPLEMENT POLLING SOLUTION

The SSE approach has failed twice with definitive proof of architectural incompatibility:

**Why Polling Is Now The Right Choice:**
1. **Database operations work 100%** - confirmed in all tests
2. **Simple and reliable** - no complex stream management
3. **Easy to debug** - standard HTTP requests
4. **Vercel-compatible** - no serverless state issues
5. **Fast implementation** - ~30 minutes vs days of debugging

**Polling Implementation Priority:**
1. **Create `/api/chat/[id]/messages` endpoint** - return messages since timestamp
2. **Add frontend polling logic** - check every 3 seconds for new messages  
3. **Add polling state management** - start/stop polling based on n8n model usage
4. **Test locally** - verify polling works without n8n
5. **Test with n8n** - verify end-to-end flow

### DEFINITIVE LOG ANALYSIS (May 23, 2025):

#### Key Logs From Vercel:

**‚úÖ `/api/chat` Function Working Correctly:**
```
[API Route] Triggering n8n workflow for chat 44a18300-955d-4012-a342-dc65b81226ba
[API Route] n8n webhook triggered, waiting for callback...
[API Route] n8n webhook responded with status 200
[API Route] Heartbeat sent for chat 44a18300-955d-4012-a342-dc65b81226ba
[API Route] Heartbeat sent for chat 44a18300-955d-4012-a342-dc65b81226ba
[API Route] Heartbeat sent for chat 44a18300-955d-4012-a342-dc65b81226ba
[API Route] Heartbeat sent for chat 44a18300-955d-4012-a342-dc65b81226ba
[API Route] Heartbeat sent for chat 44a18300-955d-4012-a342-dc65b81226ba
```

**‚úÖ `/api/n8n-callback` Function Working Correctly:**
```
[n8n-callback] POST request received
[n8n-callback] Request body: {
  "chatId": "44a18300-955d-4012-a342-dc65b81226ba",
  "responseMessage": "Yes, I got your message. What can I do for you?"
}
[n8n-callback] Saving assistant message to database...
[n8n-callback] Successfully saved assistant message
[n8n-callback] Successfully processed callback
```

**‚ùå THE SMOKING GUN:**
```
[n8n-callback] No waiting stream resolver found for chat 44a18300-955d-4012-a342-dc65b81226ba
```

#### ROOT CAUSE CONFIRMED: Vercel Serverless State Loss

**What's Happening:**
1. **`/api/chat` function** creates promise, stores resolver in `global.streamResolvers`, starts waiting
2. **`/api/chat` function continues** sending heartbeats (proves stream is alive and function is running)
3. **`/api/n8n-callback` runs in DIFFERENT serverless function instance** 
4. **New instance has empty global maps** ‚Üí `global.streamResolvers.get(chatId)` returns `undefined`
5. **Promise never resolves** ‚Üí thinking animation continues forever
6. **Database save works** ‚Üí message appears on refresh

#### ARCHITECTURAL IMPOSSIBILITY CONFIRMED

**The SSE approach cannot work on Vercel because:**
- ‚ùå **Different function instances** - each API route runs in separate serverless containers
- ‚ùå **No shared memory** - global variables don't persist between instances  
- ‚ùå **No inter-function communication** - `/api/chat` and `/api/n8n-callback` can't share state
- ‚ùå **Serverless statelessness** - fundamental incompatibility with SSE state requirements

**Evidence:**
- **6 heartbeats sent** = 3+ minutes of function execution (proves `/api/chat` staying alive)
- **"No waiting stream resolver found"** = callback runs in different instance (proves state loss)
- **Database operations work perfectly** = confirms reliable fallback exists

#### FINAL DECISION: IMPLEMENT POLLING SOLUTION IMMEDIATELY

After 3 failed SSE attempts with definitive proof of architectural incompatibility:

**Time to stop fighting Vercel's architecture and implement the simple solution that works.**

## POLLING SOLUTION IMPLEMENTATION (May 23, 2025)

### Backend Implementation - COMPLETED ‚úÖ

**Git Commit**: `Implement polling solution: revert SSE complexity, add messages API endpoint`

#### What Was Implemented:

1. **Simplified `/api/chat`** - Fire-and-forget for n8n models (returns immediately)
2. **Simplified `/api/n8n-callback`** - Just saves to database + revalidates cache  
3. **New `/api/chat/[id]/messages`** - Returns messages since timestamp for polling
4. **Updated `middleware.ts`** - Added messages endpoint to public routes for frontend access

#### Current Backend Behavior - WORKING:
- ‚úÖ Send message to n8n ‚Üí Returns OK immediately
- ‚úÖ n8n processes ‚Üí Takes 2s-12min  
- ‚úÖ n8n calls back ‚Üí Saves to database
- ‚úÖ Messages API ‚Üí Returns new messages since timestamp
- ‚úÖ Hard refresh ‚Üí Shows n8n response

### Frontend Implementation - STILL NEEDED ‚ùå

#### Critical Issue: Polling ‚â† Streaming
**The polling approach will NOT automatically work like streaming models (Sonnet/GPT-4o).**

**Streaming models**: Assistant UI framework handles everything automatically  
**N8N polling**: Requires manual frontend implementation for:
- ‚úÖ Thinking animation while polling
- ‚úÖ Polling logic (check every 3s for new messages)  
- ‚úÖ Message integration (add polled messages to chat)
- ‚úÖ State management (start/stop polling)

#### Frontend Work Required (~60 minutes):
1. **N8N model detection** - know when to start polling
2. **Polling state management** - track polling status  
3. **Polling loop** - check `/api/chat/[id]/messages` every 3s
4. **Message integration** - add polled messages to chat UI
5. **Thinking animation** - show while polling, hide when complete

**NEXT STEP: Implement frontend polling to complete the solution.**

---
